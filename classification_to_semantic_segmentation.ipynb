{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "397287fe-af07-4bca-813d-72e05e1b067f",
      "metadata": {
        "id": "397287fe-af07-4bca-813d-72e05e1b067f"
      },
      "source": [
        "# From Classification to Semantics with CNNs\n",
        "\n",
        "**Author:** [Liam O'Driscoll](https://github.com/lodriscoll)<br>\n",
        "**Date created:** 06/04/2023<br>\n",
        "**Last modified:** 06/15/2023<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preface\n",
        "\n",
        "This notebook was developed using a google compute engine, runtimes and download times may vary on your runtime of choice. You may contact the professor for free google compute engine coupons for students if you want to play around with the code without extremely long runtimes."
      ],
      "metadata": {
        "id": "dFIKyBGh1Wcz"
      },
      "id": "dFIKyBGh1Wcz"
    },
    {
      "cell_type": "markdown",
      "id": "7d383b13-5889-4f61-8fae-56687840349f",
      "metadata": {
        "id": "7d383b13-5889-4f61-8fae-56687840349f"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This exercise provides a comprehensive review of image classification then implements semantic segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db309248-fd40-43fa-9e9d-2126445780d2",
      "metadata": {
        "id": "db309248-fd40-43fa-9e9d-2126445780d2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We must first begin by importing the necessary packages. We will be using [Tensorflow](https://www.tensorflow.org/api_docs/python/tf) and Keras to implement our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7679fe73-2bd6-45c1-8299-a758d3e32360",
      "metadata": {
        "id": "7679fe73-2bd6-45c1-8299-a758d3e32360"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Resizing, Conv2DTranspose, Concatenate, UpSampling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f312583-5aa3-4401-bb25-a3eeaf90394d",
      "metadata": {
        "id": "2f312583-5aa3-4401-bb25-a3eeaf90394d"
      },
      "source": [
        "## Data\n",
        "\n",
        "For the classification portion of this exercise we will be training and evaluating using the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), a subset of the CIFAR-100 dataset. It served as a benchmark in the early era of big data, providing a challenging and diverse dataset for image classification tasks. Its large-scale nature and diverse classes contributed to advancing the field of computer vision research. Today we will be using CIFAR-10 as a benchmark to test our classification networks. Keras has already implemented a load_data() function for this operation so we will leverage this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4e694a-b5bb-4604-9e3f-bba2ff3a5b21",
      "metadata": {
        "id": "9d4e694a-b5bb-4604-9e3f-bba2ff3a5b21"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = cifar10.load_data()\n",
        "\n",
        "# Print shape of the loaded data\n",
        "print(\"Training data shape:\", x_train_raw.shape)  # (50000, 32, 32, 3)\n",
        "print(\"Training labels shape:\", y_train_raw.shape)  # (50000, 1)\n",
        "print(\"Test data shape:\", x_test_raw.shape)  # (10000, 32, 32, 3)\n",
        "print(\"Test labels shape:\", y_test_raw.shape)  # (10000, 1)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train_raw, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_raw, num_classes)\n",
        "\n",
        "# Normalize input data\n",
        "x_train = x_train_raw.astype('float32') / 255.0\n",
        "x_test = x_test_raw.astype('float32') / 255.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7e52b5a-d181-41a7-91d6-3608ba94261a",
      "metadata": {
        "id": "b7e52b5a-d181-41a7-91d6-3608ba94261a"
      },
      "source": [
        "## Visualization\n",
        "Lets take a look at a few examples by sampling the training set randomly and displaying the image with its corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c80b99-c45f-4d93-9633-cf9b08b21f2f",
      "metadata": {
        "id": "60c80b99-c45f-4d93-9633-cf9b08b21f2f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_sample_images(x, y, class_names, num_images=6):\n",
        "    # Generate random indices to select sample images from the dataset\n",
        "    indices = np.random.randint(0, len(x), num_images)\n",
        "\n",
        "    # Create a subplot grid based on the number of images\n",
        "    num_rows = (num_images + 2) // 3\n",
        "    fig, axes = plt.subplots(num_rows, 3, figsize=(8, 4 * num_rows))\n",
        "\n",
        "    # Iterate over the images and plot them with their labels\n",
        "    for i, index in enumerate(indices):\n",
        "        row = i // 3\n",
        "        col = i % 3\n",
        "\n",
        "        image = x[index]\n",
        "        label = y[index]\n",
        "        class_name = class_names[np.argmax(label)]\n",
        "\n",
        "        axes[row, col].imshow(image)\n",
        "        axes[row, col].axis('off')\n",
        "        axes[row, col].set_title(f\"Label: {class_name}\")\n",
        "\n",
        "    # Adjust the spacing and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Define the class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Plot 5 sample images with their labels\n",
        "plot_sample_images(x_train, y_train, class_names, num_images=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d0bef7-8816-4d6a-9e82-4091be9c030f",
      "metadata": {
        "id": "c9d0bef7-8816-4d6a-9e82-4091be9c030f"
      },
      "source": [
        "# Classification\n",
        "\n",
        "We can now build our first classification network. Lets start with a fully connected neural network (also called a Multilayer Perceptron Network or  MLP for short).\n",
        "\n",
        "## Fully Connected Neural Network (MLP)\n",
        "\n",
        "This model has 4 hidden layers, is optimized using SGD with momentum, and is trained for 10 epochs with a batchsize of 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea6e01f-74ba-4ad5-b623-c7d4ba62cf32",
      "metadata": {
        "id": "7ea6e01f-74ba-4ad5-b623-c7d4ba62cf32"
      },
      "outputs": [],
      "source": [
        "def build_mlp_model(input_shape=(32,32,3)):\n",
        "\n",
        "    # Build model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input shape matches the dimensions of images\n",
        "    model.add(Input(shape=(input_shape)))\n",
        "\n",
        "    # Flatten to pass into dense layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Four hidden layers\n",
        "    model.add(Dense(units=256, activation='relu'))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "\n",
        "    # Output layer with 10 units corresponding to classes with softmax activation\n",
        "    model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# define modular training function\n",
        "def train_model(model, x_train, y_train, optimizer, loss, metrics, epochs=10, batch_size=32):\n",
        "\n",
        "    # compile model with specified hyperparamers\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss,\n",
        "                  metrics=metrics)\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    #return history\n",
        "    return history\n",
        "\n",
        "# define modular evaluation function\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test loss: {loss:.4f}\")\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "mlp_model = build_mlp_model(input_shape=(32,32,3))\n",
        "\n",
        "# Use momentum SGD as optimizer and categorical_crossentropy\n",
        "# because the labels were one-hot encoded\n",
        "\n",
        "history = train_model(mlp_model, x_test, y_test,\n",
        "                   optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "evaluate_model(mlp_model, x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a763563f-f030-4593-9757-84e528dc1034",
      "metadata": {
        "id": "a763563f-f030-4593-9757-84e528dc1034"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "As we can see, our current model is struggling to fit the training set. To improve its performance, we can consider implementing the following techniques:\n",
        "\n",
        "- Adding more layers to the model, which can increase its capacity to learn complex patterns. [Regularization](https://keras.io/api/layers/regularizers/), such as L1 or L2 regularization, can also be applied to prevent overfitting.\n",
        "\n",
        "- Utilizing [batch normalization](https://keras.io/api/layers/normalization_layers/batch_normalization/), which normalizes the activations of the previous layer, helping the model to converge faster and generalize better.\n",
        "\n",
        "- Introducing [dropout layers](https://keras.io/api/layers/regularization_layers/dropout/) to randomly deactivate some neurons during training, which reduces overfitting and encourages the network to learn more robust features.\n",
        "\n",
        "- Optimizing hyperparameters using techniques like [random grid search](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/), which systematically explores the hyperparameter space to find the best combination.\n",
        "\n",
        "Let's implement these changes and see if our model improves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3142a46-3c12-4884-91cd-fb645f8cb307",
      "metadata": {
        "id": "d3142a46-3c12-4884-91cd-fb645f8cb307"
      },
      "outputs": [],
      "source": [
        "def build_better_mlp_model(input_shape=(32,32,3)):\n",
        "    # Build model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input shape matches the dimensions of images\n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    # Flatten to pass into dense layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Four hidden layers\n",
        "    model.add(Dense(units=256, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Output layer with 10 units corresponding to classes with softmax activation\n",
        "    model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "better_mlp_model = build_better_mlp_model(input_shape=(32,32,3))\n",
        "\n",
        "history = train_model(better_mlp_model, x_train, y_train,\n",
        "                      optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "evaluate_model(better_mlp_model, x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47e1d3c-5666-4d94-827f-b0a327279447",
      "metadata": {
        "id": "c47e1d3c-5666-4d94-827f-b0a327279447"
      },
      "source": [
        "### Limitations\n",
        "As we can see even with these improvements fully connected networks struggle to classify images with accuracy above 50% on the CIFAR-10 Dataset. For image classification MLPs suffer from two significant and related problems:\n",
        "\n",
        "1) **Large number of parameters:** In our case, each pixel in the 32x32 image requires a separate input neuron, resulting in an explosion of parameters as the network deepens. This leads to computational inefficiency and can cause overfitting.\n",
        "\n",
        "2) **Location Depended Pattern Recognition:** Fully connected networks can only detect patterns in the exact same x, y location where they were learned. This limitation hampers their ability to generalize and recognize patterns in different spatial locations.\n",
        "\n",
        "### Convolutional Neural Networks\n",
        "\n",
        "To address the limitations of the fully connected network, we can introduce an **inductive bias**, also known as human priors, to improve the model's capabilities. We desire improved efficiency and location indepenedtent pattern recognition. To do so we introduce the convolution operation. This new operation involves sliding windows with their own set of weights across the image which offers two main advantages:\n",
        "\n",
        "1) **Parameter sharing:** The same set of weights is used across different locations in the input, resulting in improved efficiency. This reduces the number of parameters required to learn from the data.\n",
        "\n",
        "2) **Translation invariance:** The convolution operation allows patterns to be recognized regardless of their position in the image. This enhances the model's ability to generalize and detect features at different locations.\n",
        "\n",
        "To leverage the benefits of CNNs, we construct a new model with 3 convolution and max-pooling blocks and the better MLP classifier on top based. This new architecture allows us to increase the channel depth while compressing spatial information, enabling the network to capture relevant features and hierarchies present in the data more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171c96e2-7e02-4f41-bdeb-a5022b1075c1",
      "metadata": {
        "id": "171c96e2-7e02-4f41-bdeb-a5022b1075c1"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model():\n",
        "\n",
        "    # create conv model\n",
        "    conv_model = Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    conv_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32,32,3)))\n",
        "    conv_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    conv_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 2\n",
        "    conv_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    conv_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    conv_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Block 3\n",
        "    conv_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    conv_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    conv_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # the return model\n",
        "    model = Sequential()\n",
        "\n",
        "    # stack better MLP classifier on top of conv model\n",
        "    # with input shape as output shape of last conv block\n",
        "    model.add(conv_model)\n",
        "    model.add(build_better_mlp_model(input_shape=conv_model.output.shape[1:]))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "cnn_model = build_cnn_model()\n",
        "\n",
        "# same optimizer and loss\n",
        "history = train_model(cnn_model, x_train, y_train,\n",
        "                      optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "evaluate_model(cnn_model, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35f2c0a0-373b-4971-920a-1241d365a84d",
      "metadata": {
        "tags": [],
        "id": "35f2c0a0-373b-4971-920a-1241d365a84d"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "The comparison between the accuracies of the CNN and fully connected network clearly demonstrates the superiority of CNN in performance. As discussed earlier, the convolution operation provides more robust generalization for image data.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "Although the CNN is a significant improvement over the MLP classifier, it still has inherent limitations. In fact, these limitations are direct byproducts of our inductive bias. One of these limitations is the lack of access to the context of the original image in later layers. This issue arises due to the simultaneous spatial compression and expansion of channel depth. To overcome this limitation, a concept called residual layers is introduced.\n",
        "\n",
        "**Residual layers** act as skip connections between a layer and its previous layer, allowing the propagation of context throughout the network when it is optimal to do so.\n",
        "\n",
        "## Go Larger and Deeper - ResNet50\n",
        "\n",
        "ResNet50 is a deep convolutional neural network architecture that significantly improves the performance and training of CNNs. It introduces the concept of residual learning, where residual layers are added to the network to mitigate the issue of context loss in deeper layers. The ResNet50 architecture consists of 50 layers, including convolutional layers, pooling layers, and fully connected layers. It has achieved state-of-the-art results on various image classification tasks.\n",
        "\n",
        "The ResNet50 model was originally trained on the [ImageNet](https://www.image-net.org/) dataset, the second large-scale dataset that we will be looking at. ImageNet contains millions of labeled images from thousands of categories and serves as a benchmark dataset for training and evaluating deep learning models for image classification. The dataset spans a wide range of visual concepts, including objects, animals, and scenes.\n",
        "\n",
        "Pre-training ResNet50 on ImageNet allows the model to extract meaningful and discriminative features from images. These learned features can then be fine-tuned or transferred to other datasets, including those with specific classes like the CIFAR-10 dataset.\n",
        "\n",
        "By incorporating residual layers and leveraging their skip connections, ResNet50 allows the network to learn more effectively and capture both low-level and high-level features in an image. This architecture has been widely adopted and serves as a powerful tool for various computer vision tasks.\n",
        "\n",
        "For further details and implementation specifics, please refer to the [ResNet50 paper](https://arxiv.org/abs/1512.03385). We will adapt this architecture to our specific needs, in this case classifying the CIFAR-10 dataset. We will use a transfer learning approach -- the following code should look familiar if you took CSE144.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8455aea9-fb7c-4a0b-b5db-d2e14db7a1ed",
      "metadata": {
        "id": "8455aea9-fb7c-4a0b-b5db-d2e14db7a1ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_transfer_model():\n",
        "\n",
        "    # donwload resnet architecture without top (the original classification layers whith shape 1000)\n",
        "    # with imagenet weights, and input shape that matches CIFAR-10\n",
        "    res_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(32, 32, 3))\n",
        "\n",
        "    # freeze all weights in res_model because we want to use transfer learning\n",
        "    for layer in res_model.layers:\n",
        "       layer.trainable = False\n",
        "\n",
        "    # build sequential model with mlp classifier stacked on top of resnet\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(res_model)\n",
        "    model.add(build_better_mlp_model(input_shape=res_model.output.shape[1:]))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# keras' resnet implementation requires you to preprocesses the input\n",
        "# so we define a wrapper function for train_model\n",
        "#\n",
        "# [RGB] -> [BGR]\n",
        "# zero-centers each channel wrt imagenet without scaling\n",
        "#\n",
        "# link: https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50\n",
        "#\n",
        "def train_transfer_model(transfer_model, x_train, y_train, optimizer, loss, metrics, epochs=10, batch_size=32):\n",
        "\n",
        "    # preprocess data as specified in docs\n",
        "    x_train_proc = preprocess_input(np.copy(x_train))\n",
        "\n",
        "    # use modular train_model function from earlier to get history\n",
        "    history = train_model(transfer_model, x_train_proc, y_train, optimizer, loss, metrics, epochs, batch_size)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "transfer_model = build_transfer_model()\n",
        "\n",
        "history = train_transfer_model(transfer_model, x_train, y_train,\n",
        "                              optimizer=SGD(learning_rate=0.001, momentum=.9),\n",
        "                              loss='categorical_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "evaluate_model(transfer_model, x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489015e8-cb58-4ea6-ad0d-aa78005388f0",
      "metadata": {
        "id": "489015e8-cb58-4ea6-ad0d-aa78005388f0"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "As we can see, ResNet offers another significant improvement over the traditional CNN model. The addition of skip connections, also known as residual connections, plays a crucial role in enhancing the model's performance. These skip connections allow for the direct flow of information from earlier layers to later layers, mitigating the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) and enabling the network to learn more effectively. By preserving and propagating gradients, the skip connections help to alleviate the challenges of training very deep networks. Additionally, the skip connections introduce shortcut paths that bypass a few layers, enabling the model to capture both low-level and high-level features more efficiently. This improved information flow and feature preservation contribute to the enhanced performance of ResNet on various tasks, including image classification.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "One of the main limitations of ResNet is its computational cost. The deep structure of ResNet, especially in variants like ResNet50, requires significant computational resources and training time. Training such models on large-scale datasets or with limited computing power can be challenging. Additionally, the large number of parameters in ResNet increases the risk of overfitting, especially when the training dataset is small or lacks diversity. Regularization techniques such as dropout and weight decay can help mitigate overfitting to some extent.\n",
        "\n",
        "While ResNet has shown impressive performance, it has a limitation in terms of contextual information utilization. The skip connections in ResNet only propagate information from the previous layer to the next layer. As a result, later layers may not have direct access to the entire context of the original image, which can limit their ability to effectively utilize global information. This limitation can impact the model's performance, particularly in tasks that require capturing long-range dependencies or contextual understanding.\n",
        "\n",
        "##### Side Note\n",
        "\n",
        "[DenseNet](https://arxiv.org/abs/1608.06993) is an architecture that addresses the limitation of contextual information utilization in ResNet by introducing dense connections. In DenseNet, each layer is connected to all previous layers, allowing for direct information flow from all preceding layers. This design enables dense connections to propagate richer and more comprehensive information throughout the network. By incorporating dense connections, DenseNet facilitates better information flow, gradient propagation, and feature reuse across different layers. This architecture is beyond the scope of this exercise so let us move on from classification.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantics\n",
        "\n",
        "After comprehensively reviewing classification we are now equipped with the tools to move onto a more challenging beast: semantic segmentation. As we learned, Image classification is the task of assigning a single label or class to an entire image. The goal is to determine what is present in the image and categorize it into one of several predefined classes. The output of image classification is a single class label or a probability distribution over class labels.\n",
        "\n",
        "On the other hand, semantic segmentation is the task of pixel-level labeling, where the objective is to assign a class label to each individual pixel in an image. The goal is to classify and differentiate the various objects or regions within an image. Semantic segmentation provides a detailed understanding of the scene by segmenting the image into different meaningful regions. For example, in an image containing a dog and a cat, semantic segmentation would aim to assign a specific label to each pixel belonging to the dog and the cat separately. The output of semantic segmentation is a pixel-wise classification map, where each pixel is assigned a class label.\n",
        "\n",
        "## Data\n",
        "\n",
        "To achieve this let's take a look at the third, and final large scale dataset: [Cityscapes](https://paperswithcode.com/dataset/cityscapes). This dataset is widely used in semantic segmentation and serves as a benchmark for most modern day driving computer vision systems. We will download a subset of this dataset using the [kaggle API](https://www.kaggle.com/general/156610) to streamline this process. In order to run these cells you must download your API token (a json file) from your account tab in Kaggle and upload it to this notebook. Further instructions and clarifications are available at that link."
      ],
      "metadata": {
        "id": "SsscNzEQ2lqc"
      },
      "id": "SsscNzEQ2lqc"
    },
    {
      "cell_type": "code",
      "source": [
        "# download token and upload to this notebook's working directory BEFORE running the following cell"
      ],
      "metadata": {
        "id": "92Txhxx63b4v"
      },
      "id": "92Txhxx63b4v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download dansbecker/cityscapes-image-pairs\n",
        "\n",
        "! unzip -q cityscapes-image-pairs.zip"
      ],
      "metadata": {
        "id": "HPN8tDS-3o4Q"
      },
      "id": "HPN8tDS-3o4Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Preprocessing\n",
        "\n",
        "We must now split the concatenated images into our x_train (raw image) and y_train (labeled mask)"
      ],
      "metadata": {
        "id": "D29TaiEQ41pr"
      },
      "id": "D29TaiEQ41pr"
    },
    {
      "cell_type": "code",
      "source": [
        "# define ID map and category map for each pixel (from city scapes documentation)\n",
        "\n",
        "id_map = {\n",
        "    0: (0, 0, 0), # unlabelled\n",
        "    1: (111, 74,  0), #static\n",
        "    2: ( 81,  0, 81), #ground\n",
        "    3: (128, 64,127), #road\n",
        "    4: (244, 35,232), #sidewalk\n",
        "    5: (250,170,160), #parking\n",
        "    6: (230,150,140), #rail track\n",
        "    7: (70, 70, 70), #building\n",
        "    8: (102,102,156), #wall\n",
        "    9: (190,153,153), #fence\n",
        "    10: (180,165,180), #guard rail\n",
        "    11: (150,100,100), #bridge\n",
        "    12: (150,120, 90), #tunnel\n",
        "    13: (153,153,153), #pole\n",
        "    14: (153,153,153), #polegroup\n",
        "    15: (250,170, 30), #traffic light\n",
        "    16: (220,220,  0), #traffic sign\n",
        "    17: (107,142, 35), #vegetation\n",
        "    18: (152,251,152), #terrain\n",
        "    19: ( 70,130,180), #sky\n",
        "    20: (220, 20, 60), #person\n",
        "    21: (255,  0,  0), #rider\n",
        "    22: (  0,  0,142), #car\n",
        "    23: (  0,  0, 70), #truck\n",
        "    24: (  0, 60,100), #bus\n",
        "    25: (  0,  0, 90), #caravan\n",
        "    26: (  0,  0,110), #trailer\n",
        "    27: (  0, 80,100), #train\n",
        "    28: (  0,  0,230), #motorcycle\n",
        "    29: (119, 11, 32), #bicycle\n",
        "    30: (  0,  0,142) #license plate\n",
        "}\n",
        "\n",
        "category_map = {\n",
        "    0: 0,\n",
        "    1: 0,\n",
        "    2: 0,\n",
        "    3: 1,\n",
        "    4: 1,\n",
        "    5: 1,\n",
        "    6: 1,\n",
        "    7: 2,\n",
        "    8: 2,\n",
        "    9: 2,\n",
        "    10: 2,\n",
        "    11: 2,\n",
        "    12: 2,\n",
        "    13: 3,\n",
        "    14: 3,\n",
        "    15: 3,\n",
        "    16: 3,\n",
        "    17: 4,\n",
        "    18: 4,\n",
        "    19: 5,\n",
        "    20: 6,\n",
        "    21: 6,\n",
        "    22: 7,\n",
        "    23: 7,\n",
        "    24: 7,\n",
        "    25: 7,\n",
        "    26: 7,\n",
        "    27: 7,\n",
        "    28: 7,\n",
        "    29: 7,\n",
        "    30: 7\n",
        "}"
      ],
      "metadata": {
        "id": "TkCwgbsX5Ofq"
      },
      "id": "TkCwgbsX5Ofq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path):\n",
        "    # Open the image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Crop and resize two regions of the image\n",
        "    region1 = image.crop((0, 0, 256, 256)).resize((128, 128))\n",
        "    region2 = image.crop((256, 0, 512, 256)).resize((128, 128))\n",
        "\n",
        "    # Convert region1 to a numpy array and normalize its values\n",
        "    normalized_region1 = np.array(region1) / 255.\n",
        "\n",
        "    # Convert region2 to a numpy array\n",
        "    region2_array = np.array(region2)\n",
        "\n",
        "    # Create an empty mask with the same shape as region2\n",
        "    mask = np.zeros(shape=(region2_array.shape[0], region2_array.shape[1]), dtype=np.uint32)\n",
        "\n",
        "    # Iterate over each pixel in region2\n",
        "    for row in range(region2_array.shape[0]):\n",
        "        for col in range(region2_array.shape[1]):\n",
        "            pixel_value = region2_array[row, col, :]\n",
        "            final_key = None\n",
        "            final_distance = None\n",
        "\n",
        "            # Find the closest matching key in the id_map dictionary\n",
        "            for key, value in id_map.items():\n",
        "                distance = np.sum(np.sqrt(np.power(pixel_value - value, 2)))\n",
        "\n",
        "                # Update the final_key and final_distance if it's the first iteration\n",
        "                if final_key is None:\n",
        "                    final_distance = distance\n",
        "                    final_key = key\n",
        "                # Update the final_key and final_distance if the current distance is smaller\n",
        "                elif distance < final_distance:\n",
        "                    final_distance = distance\n",
        "                    final_key = key\n",
        "\n",
        "            # Assign the final_key to the corresponding pixel in the mask\n",
        "            mask[row, col] = final_key\n",
        "\n",
        "    # Reshape the mask to have an additional dimension\n",
        "    mask = np.reshape(mask, (mask.shape[0], mask.shape[1], 1))\n",
        "\n",
        "    # Clean up the region2 variable\n",
        "    del region2_array\n",
        "\n",
        "    # Return the preprocessed image and mask\n",
        "    return normalized_region1, mask\n",
        "\n",
        "def get_train_val_split_from_filenames(train_path, val_path):\n",
        "    # Initialize empty lists for training and validation data\n",
        "    train_images = []\n",
        "    train_masks = []\n",
        "    val_images = []\n",
        "    val_masks = []\n",
        "\n",
        "    # Process training images and masks\n",
        "    for file in tqdm(os.listdir(train_path)):\n",
        "        image, mask = preprocess(f\"{train_path}/{file}\")\n",
        "        train_images.append(image)\n",
        "        train_masks.append(mask)\n",
        "\n",
        "    # Process validation images and masks\n",
        "    for file in tqdm(os.listdir(val_path)):\n",
        "        image, mask = preprocess(f\"{val_path}/{file}\")\n",
        "        val_images.append(image)\n",
        "        val_masks.append(mask)\n",
        "\n",
        "    # Return the training and validation data\n",
        "    return np.array(train_images), np.array(train_masks), np.array(val_images), np.array(val_masks)"
      ],
      "metadata": {
        "id": "ggG9892S57vG"
      },
      "id": "ggG9892S57vG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will take a while\n",
        "\n",
        "x_train, y_train, x_val, y_val = get_train_val_split_from_filenames(\"cityscapes_data/train/\", \"cityscapes_data/val/\")"
      ],
      "metadata": {
        "id": "REH8Gzh26WkZ"
      },
      "id": "REH8Gzh26WkZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Convolutional Network (FCN)\n",
        "\n",
        "Lets begin by creating our first fully convolutional network. We will use the ResNet architecture as our base model. It will be pretrained on the same ImageNet weights and like last time, we will remove the top so we can modify the architecture to our needs."
      ],
      "metadata": {
        "id": "6zHNieAK6j25"
      },
      "id": "6zHNieAK6j25"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fcn_model():\n",
        "\n",
        "    res_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(128, 128, 3))\n",
        "\n",
        "    # freeze all weights in res_model because we want to use transfer learning\n",
        "    for layer in res_model.layers:\n",
        "       layer.trainable = False\n",
        "\n",
        "    # build sequential model with semantic segmentation classifier\n",
        "    # stacked on top of resnet\n",
        "    model = Sequential()\n",
        "    model.add(res_model)\n",
        "\n",
        "    # want 7 filters for the 7 classes\n",
        "    model.add(Conv2D(7, (1, 1), activation='softmax', padding='same'))\n",
        "\n",
        "    # resize to shape of input images so we can compute loss\n",
        "    model.add(Resizing(128, 128))\n",
        "\n",
        "    return model\n",
        "\n",
        "fcn_model = build_fcn_model()\n",
        "\n",
        "# have to use driver function again because we are using resnet as network backbone\n",
        "history = train_transfer_model(fcn_model, x_train, y_train,\n",
        "                              optimizer=SGD(learning_rate=0.001, momentum=.9),\n",
        "                              loss='categorical_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "# use validation data as test set for semantics\n",
        "evaluate_model(fcn_model, x_val, y_val)\n"
      ],
      "metadata": {
        "id": "vnV5v7oR677E"
      },
      "id": "vnV5v7oR677E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Better FCN\n",
        "\n",
        "The previous model is only able to provide coarse segmentation masks. This is due to decision to upsample the resulting feature map without any modifications. Resizing the feature map in this way removes all fine details from the feature map. To improve upon this, we introduce the [transpose convolution](https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967) operation. This allows the network to learn how to upsample the coarse segmentation mask to provide fine grain details. Let's implement these changes."
      ],
      "metadata": {
        "id": "xebxD_77AH9h"
      },
      "id": "xebxD_77AH9h"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_better_fcn_model():\n",
        "    res_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze all weights in res_model because we want to use transfer learning\n",
        "    for layer in res_model.layers:\n",
        "       layer.trainable = False\n",
        "\n",
        "    # Build sequential model with better semantic segmentation classifier\n",
        "    # stacked on top of ResNet\n",
        "    model = Sequential()\n",
        "    model.add(res_model)\n",
        "\n",
        "    # Use inverse convolution (Conv2DTranspose) instead of resizing\n",
        "    model.add(Conv2DTranspose(256, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n",
        "    model.add(Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n",
        "    model.add(Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n",
        "\n",
        "    # Use Conv2D to reduce to the desired number of classes\n",
        "    model.add(Conv2D(7, (1, 1), activation='softmax', padding='same'))\n",
        "\n",
        "    return model\n",
        "\n",
        "better_fcn_model = build_better_fcn_model()\n",
        "\n",
        "history = train_transfer_model(better_fcn_model, x_train, y_train,\n",
        "                              optimizer=SGD(learning_rate=0.001, momentum=.9),\n",
        "                              loss='categorical_crossentropy',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "evaluate_model(better_fcn_model, x_val, y_val)\n"
      ],
      "metadata": {
        "id": "LmjmM6kMAIDv"
      },
      "id": "LmjmM6kMAIDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-Net\n",
        "\n",
        "These results are impressive but our better FCN suffers from one large and inherent issue. Since we decided to freeze the weights and to not include the top, our segmentation classifier loses access to context that was available in earlier layers. We can solve this issue by using a technique introduced in the ResNet architecture. By adding residual connections between early and later convolutional layers with the same shape, we can increase performance by giving the model more context in later layers. Further information can be found in the [original U-NET paper](https://arxiv.org/abs/1505.04597)"
      ],
      "metadata": {
        "id": "RWa_YQna6-jX"
      },
      "id": "RWa_YQna6-jX"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Contraction path - in better FCN we used resnet to accomplish this\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D()(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D()(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D()(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D()(conv4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    # Expansion Path - this is the semantic classifier (or top) from previous network\n",
        "    up6 = Conv2D(512, 2, activation='relu', padding='same')(UpSampling2D()(conv5))\n",
        "    merge6 = Concatenate()([conv4, up6])\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation='relu', padding='same')(UpSampling2D()(conv6))\n",
        "    merge7 = Concatenate()([conv3, up7])\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation='relu', padding='same')(UpSampling2D()(conv7))\n",
        "    merge8 = Concatenate()([conv2, up8])\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D()(conv8))\n",
        "    merge9 = Concatenate()([conv1, up9])\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "unet_model = build_unet_model()\n",
        "\n",
        "history = train_model(unet_model, x_train, y_train,\n",
        "                      optimizer=SGD(learning_rate=0.001, momentum=.9),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "evaluate_model(unet_model, x_val, y_val)\n"
      ],
      "metadata": {
        "id": "jPdv0ohl7qqZ"
      },
      "id": "jPdv0ohl7qqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "You have now seen how to implement image classification and semantic segmentation using convolutional networks and transfer learning. You have also been exposed to modular network design and three large scale benchmark datasets. In the classification section, we learned how to assign a probility distribution for the entire image. On the other hand, in the semantic segmentation section we learned how to assign a probability distribution for each pixel in the input image. You are now equipped with the tools to tackle more advanced computer vision tasks."
      ],
      "metadata": {
        "id": "8dPqaFbsElEe"
      },
      "id": "8dPqaFbsElEe"
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-cpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}